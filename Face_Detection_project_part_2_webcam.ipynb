{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from keras.models import load_model\n","from PIL import Image\n","import pickle\n","import matplotlib.pyplot as plt\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import PIL\n","import io"],"metadata":{"id":"O6UYIxeF-HOq","executionInfo":{"status":"ok","timestamp":1738505493228,"user_tz":-360,"elapsed":13173,"user":{"displayName":"Abdullah Al Noman","userId":"14829655779709154281"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVMkCDbS7K4a","outputId":"ba94eb8d-6f19-46a3-fe37-fbe7ec8b50c4","executionInfo":{"status":"ok","timestamp":1738505516312,"user_tz":-360,"elapsed":23086,"user":{"displayName":"Abdullah Al Noman","userId":"14829655779709154281"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IsKCdLx56qW-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4dcc181-2f4f-43bd-8d8e-69007b6bf8d9","executionInfo":{"status":"ok","timestamp":1738505585804,"user_tz":-360,"elapsed":4631,"user":{"displayName":"Abdullah Al Noman","userId":"14829655779709154281"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['loss', 'compile_metrics']\n"]}],"source":["\n","\n","# Load the trained model\n","model = load_model('/content/drive/MyDrive/CVPR/CVPR_PROJECT/Dataset.keras')\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","# Recompile with explicit metrics\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n","\n","# Check if metrics are now available\n","print(model.metrics_names)\n"]},{"cell_type":"code","source":["model.evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"1FJHNqSWGKvP","outputId":"86666e91-fd80-457b-d225-1e7421177861","executionInfo":{"status":"ok","timestamp":1738505595448,"user_tz":-360,"elapsed":433,"user":{"displayName":"Abdullah Al Noman","userId":"14829655779709154281"}}},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method TensorFlowTrainer.evaluate of <Sequential name=sequential, built=True>>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>keras.src.backend.tensorflow.trainer.TensorFlowTrainer.evaluate</b><br/>def evaluate(x=None, y=None, batch_size=None, verbose=&#x27;auto&#x27;, sample_weight=None, steps=None, callbacks=None, return_dict=False, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py</a>Returns the loss value &amp; metrics values for the model in test mode.\n","\n","Computation is done in batches (see the `batch_size` arg.)\n","\n","Args:\n","    x: Input data. It can be:\n","        - A NumPy array (or array-like), or a list of arrays\n","        (in case the model has multiple inputs).\n","        - A backend-native tensor, or a list of tensors\n","        (in case the model has multiple inputs).\n","        - A dict mapping input names to the corresponding array/tensors,\n","        if the model has named inputs.\n","        - A `keras.utils.PyDataset` returning `(inputs, targets)` or\n","        `(inputs, targets, sample_weights)`.\n","        - A `tf.data.Dataset` yielding `(inputs, targets)` or\n","        `(inputs, targets, sample_weights)`.\n","        - A `torch.utils.data.DataLoader` yielding `(inputs, targets)`\n","        or `(inputs, targets, sample_weights)`.\n","        - A Python generator function yielding `(inputs, targets)` or\n","        `(inputs, targets, sample_weights)`.\n","    y: Target data. Like the input data `x`, it can be either NumPy\n","        array(s) or backend-native tensor(s). If `x` is a\n","        `keras.utils.PyDataset`, `tf.data.Dataset`,\n","        `torch.utils.data.DataLoader` or a Python generator function,\n","        `y` should not be specified since targets will be obtained from\n","        `x`.\n","    batch_size: Integer or `None`.\n","        Number of samples per batch of computation.\n","        If unspecified, `batch_size` will default to 32.\n","        Do not specify the `batch_size` if your input data `x` is a\n","        `keras.utils.PyDataset`, `tf.data.Dataset`,\n","        `torch.utils.data.DataLoader` or Python generator function\n","        since they generate batches.\n","    verbose: `&quot;auto&quot;`, 0, 1, or 2. Verbosity mode.\n","        0 = silent, 1 = progress bar, 2 = single line.\n","        `&quot;auto&quot;` becomes 1 for most cases.\n","        Note that the progress bar is not\n","        particularly useful when logged to a file, so `verbose=2` is\n","        recommended when not running interactively\n","        (e.g. in a production environment). Defaults to `&quot;auto&quot;`.\n","    sample_weight: Optional NumPy array or tensor of weights for\n","        the training samples, used for weighting the loss function\n","        (during training only). You can either pass a flat (1D)\n","        NumPy array or tensor with the same length as the input samples\n","        (1:1 mapping between weights and samples), or in the case of\n","        temporal data, you can pass a 2D NumPy array or tensor with\n","        shape `(samples, sequence_length)` to apply a different weight\n","        to every timestep of every sample.\n","        This argument is not supported when `x` is a\n","        `keras.utils.PyDataset`, `tf.data.Dataset`,\n","        `torch.utils.data.DataLoader` or Python generator function.\n","        Instead, provide `sample_weights` as the third element of `x`.\n","        Note that sample weighting does not apply to metrics specified\n","        via the `metrics` argument in `compile()`. To apply sample\n","        weighting to your metrics, you can specify them via the\n","        `weighted_metrics` in `compile()` instead.\n","    steps: Integer or `None`.\n","        Total number of steps (batches of samples) to draw before\n","        declaring the evaluation round finished. If `steps` is `None`,\n","        it will run until `x` is exhausted. In the case of an infinitely\n","        repeating dataset, it will run indefinitely.\n","    callbacks: List of `keras.callbacks.Callback` instances.\n","        List of callbacks to apply during evaluation.\n","    return_dict: If `True`, loss and metric results are returned as a\n","        dict, with each key being the name of the metric.\n","        If `False`, they are returned as a list.\n","\n","Returns:\n","    Scalar test loss (if the model has a single output and no metrics)\n","    or list of scalars (if the model has multiple outputs\n","    and/or metrics). The attribute `model.metrics_names` will give you\n","    the display labels for the scalar outputs.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 427);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["model.summary()\n","\n","\n","# Ensure to print the model input shape for verification\n","print(\"Model input shape:\", model.input_shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"vhyZwyZV7nqc","outputId":"b77b9636-dcd8-47fc-dcc6-5e6b3f2bfb9f","executionInfo":{"status":"ok","timestamp":1738505602482,"user_tz":-360,"elapsed":439,"user":{"displayName":"Abdullah Al Noman","userId":"14829655779709154281"}}},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m36,992\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m115200\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │       \u001b[38;5;34m7,372,864\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m195\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,992</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">115200</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">7,372,864</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,410,947\u001b[0m (28.27 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,410,947</span> (28.27 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,410,947\u001b[0m (28.27 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,410,947</span> (28.27 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Model input shape: (None, 128, 128, 3)\n"]}]},{"cell_type":"code","source":["# Preprocessing to match model's input size\n","def preprocess_face(face, target_shape):\n","    face_resized = cv2.resize(face, (target_shape[1], target_shape[2]))  # Resize to model input shape\n","    face_normalized = face_resized / 255.0  # Normalize to [0, 1]\n","    face_array = np.expand_dims(face_normalized, axis=0)  # Add batch dimension\n","    return face_array\n"],"metadata":{"id":"mEiZUZT7VT5T","executionInfo":{"status":"ok","timestamp":1738505611544,"user_tz":-360,"elapsed":451,"user":{"displayName":"Abdullah Al Noman","userId":"14829655779709154281"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Function to handle predictions\n","def predict_label(face_array):\n","    predictions = model.predict(face_array)\n","    confidence = np.max(predictions)  # Get the maximum probability\n","    label_index = np.argmax(predictions)  # Get the index of the highest probability class\n","    return label_index, confidence\n"],"metadata":{"id":"z2lst9JdVWvI","executionInfo":{"status":"ok","timestamp":1738505620263,"user_tz":-360,"elapsed":439,"user":{"displayName":"Abdullah Al Noman","userId":"14829655779709154281"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#**Camera** **ON**"],"metadata":{"id":"9-5X0wi_AH1Z"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from base64 import b64decode, b64encode\n","import PIL.Image\n","import io\n","import pickle\n","from IPython.display import display, Javascript\n","\n","# Load the Haar Cascade for face detection\n","face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","# Load the training data (X and y) and class labels\n","pickle_in = open(\"/content/drive/My Drive/categories.pkl\", \"rb\")\n","data_load = pickle.load(pickle_in)  # Images\n","\n","pickle_in = open(\"/content/drive/My Drive/categories.pkl\", \"rb\")\n","label_encoder= pickle.load(pickle_in)  # Labels\n","\n","# Load class labels (names)\n","with open('/content/drive/MyDrive/categories.pkl', 'rb') as f:\n","    class_labels = pickle.load(f)\n","\n","print(\"Loaded Class Labels:\", class_labels)\n","print(\"Loaded Labels:\", class_labels)\n","\n","\n","\n","# Function to preprocess the face for the model\n","def preprocess_face(face, target_shape):\n","    face_resized = cv2.resize(face, (target_shape[1], target_shape[2]))\n","    face_normalized = face_resized / 255.0\n","    face_array = np.expand_dims(face_normalized, axis=0)  # Add batch dimension\n","    if len(model.input_shape) == 2:  # If the model expects flattened input\n","        face_array = face_array.reshape((1, -1))  # Flatten the array\n","    return face_array\n","\n","# Function to handle predictions\n","def predict_label(face_array):\n","    predictions = model.predict(face_array, verbose=0)  # Set verbose=0 to suppress output\n","    confidence = np.max(predictions)  # Get the maximum confidence\n","    label_index = np.argmax(predictions)  # Get the index of the highest confidence class\n","\n","    return label_index, confidence\n","\n","\n","# Function to convert JavaScript object to OpenCV image\n","def js_to_image(js_reply):\n","    image_bytes = b64decode(js_reply.split(',')[1])\n","    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","    img = cv2.imdecode(jpg_as_np, flags=1)\n","    return img\n","\n","# Function to convert OpenCV bounding box to base64 for overlay\n","def bbox_to_bytes(bbox_array):\n","    # Ensure bbox_array is in RGBA format\n","    if len(bbox_array.shape) == 3 and bbox_array.shape[2] != 4:\n","        bbox_array = cv2.cvtColor(bbox_array, cv2.COLOR_BGR2BGRA)  # Convert to BGRA if not already\n","\n","    # Convert NumPy array to PIL Image\n","    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","\n","    # Create an in-memory byte buffer\n","    iobuf = io.BytesIO()\n","\n","    # Save the image as PNG to the byte buffer\n","    bbox_PIL.save(iobuf, format='PNG')  # Use PNG to retain transparency\n","\n","    # Convert to base64 string and return\n","    bbox_bytes = 'data:image/png;base64,{}'.format(str(b64encode(iobuf.getvalue()), 'utf-8'))\n","    return bbox_bytes\n","\n","\n","# JavaScript for video stream\n","def video_stream():\n","    js = Javascript('''\n","        var video;\n","        var div = null;\n","        var stream;\n","        var captureCanvas;\n","        var imgElement;\n","        var labelElement;\n","\n","        var pendingResolve = null;\n","        var shutdown = false;\n","\n","        function removeDom() {\n","            stream.getVideoTracks()[0].stop();\n","            video.remove();\n","            div.remove();\n","            video = null;\n","            div = null;\n","            stream = null;\n","            imgElement = null;\n","            captureCanvas = null;\n","            labelElement = null;\n","        }\n","\n","        function onAnimationFrame() {\n","            if (!shutdown) {\n","                window.requestAnimationFrame(onAnimationFrame);\n","            }\n","            if (pendingResolve) {\n","                var result = \"\";\n","                if (!shutdown) {\n","                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","                    result = captureCanvas.toDataURL('image/jpeg', 0.8);\n","                }\n","                var lp = pendingResolve;\n","                pendingResolve = null;\n","                lp(result);\n","            }\n","        }\n","\n","        async function createDom() {\n","            if (div !== null) {\n","                return stream;\n","            }\n","\n","            div = document.createElement('div');\n","            div.style.border = '2px solid black';\n","            div.style.padding = '3px';\n","            div.style.width = '100%';\n","            div.style.maxWidth = '600px';\n","            document.body.appendChild(div);\n","\n","            const modelOut = document.createElement('div');\n","            modelOut.innerHTML = \"Status:\";\n","            labelElement = document.createElement('span');\n","            labelElement.innerText = 'No data';\n","            labelElement.style.fontWeight = 'bold';\n","            modelOut.appendChild(labelElement);\n","            div.appendChild(modelOut);\n","\n","            video = document.createElement('video');\n","            video.style.display = 'block';\n","            video.width = div.clientWidth - 6;\n","            video.setAttribute('playsinline', '');\n","            video.onclick = () => { shutdown = true; };\n","            stream = await navigator.mediaDevices.getUserMedia(\n","                {video: { facingMode: \"environment\"}});\n","            div.appendChild(video);\n","\n","            imgElement = document.createElement('img');\n","            imgElement.style.position = 'absolute';\n","            imgElement.style.zIndex = 1;\n","            imgElement.onclick = () => { shutdown = true; };\n","            div.appendChild(imgElement);\n","\n","            const instruction = document.createElement('div');\n","            instruction.innerHTML =\n","                '' +\n","                'When finished, click \"S\" to stop this demo';\n","            div.appendChild(instruction);\n","            instruction.onclick = () => { shutdown = true; };\n","\n","            // Add key press event listener\n","            window.addEventListener('keydown', function(e) {\n","                if (e.key === 's' || e.key === 'S') {\n","                    shutdown = true;  // Stop the video stream on 'S' key press\n","                }\n","            });\n","\n","            video.srcObject = stream;\n","            await video.play();\n","\n","            captureCanvas = document.createElement('canvas');\n","            captureCanvas.width = 640;\n","            captureCanvas.height = 480;\n","            window.requestAnimationFrame(onAnimationFrame);\n","\n","            return stream;\n","        }\n","\n","        async function stream_frame(label, imgData) {\n","            if (shutdown) {\n","                removeDom();\n","                shutdown = false;\n","                return '';\n","            }\n","\n","            var preCreate = Date.now();\n","            stream = await createDom();\n","\n","            var preShow = Date.now();\n","            if (label != \"\") {\n","                labelElement.innerHTML = label;\n","            }\n","\n","            if (imgData != \"\") {\n","                var videoRect = video.getClientRects()[0];\n","                imgElement.style.top = videoRect.top + \"px\";\n","                imgElement.style.left = videoRect.left + \"px\";\n","                imgElement.style.width = videoRect.width + \"px\";\n","                imgElement.style.height = videoRect.height + \"px\";\n","                imgElement.src = imgData;\n","            }\n","\n","            var preCapture = Date.now();\n","            var result = await new Promise(function(resolve, reject) {\n","                pendingResolve = resolve;\n","            });\n","            shutdown = false;\n","\n","            return {'create': preShow - preCreate,\n","                    'show': preCapture - preShow,\n","                    'capture': Date.now() - preCapture,\n","                    'img': result};\n","        }\n","    ''')\n","    display(js)\n","\n","def video_frame(label, bbox):\n","    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","    return data\n","\n","# Start video stream\n","video_stream()\n","label_html = 'Capturing...'\n","bbox = ''\n","count = 0\n","\n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # Convert JS response to OpenCV image\n","    img = js_to_image(js_reply[\"img\"])\n","    bbox_array = np.zeros([480, 640, 3], dtype=np.uint8)  # Use 3 channels for BGR format\n","\n","    # Convert image to grayscale for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n","\n","    # Draw bounding box and label on the image\n","    for (x, y, w, h) in faces:\n","        # Extract and preprocess the detected face\n","        face = img[y:y+h, x:x+w]\n","        face_array = preprocess_face(face, target_shape=model.input_shape)\n","\n","        # Predict the class and confidence\n","        label_index, confidence = predict_label(face_array)\n","\n","        # Determine the label based on confidence\n","        if confidence > 0.9:  # Lowered confidence threshold for debugging\n","            label = f\"{class_labels[label_index]} ({confidence*100:.2f}%)\"\n","            color = (0, 255, 0)  # Green for recognized\n","        else:\n","            label = \"Unknown\"\n","            color = (0, 0, 255)  # Red for unknown\n","\n","        # Draw the bounding box and label on the image\n","        cv2.rectangle(bbox_array, (x, y), (x+w, y+h), color, 2)\n","        cv2.putText(bbox_array, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n","\n","\n","    # Convert bbox_array to RGB format if not already\n","    bbox_array = cv2.cvtColor(bbox_array, cv2.COLOR_BGR2RGB)\n","\n","    # Create an alpha channel for transparency\n","    # Reduce the alpha value for partial transparency (e.g., 128 for 50% transparency)\n","    alpha_channel = np.full(bbox_array.shape[:2], 128, dtype=np.uint8)\n","\n","    # Add the alpha channel to the bbox_array\n","    bbox_array_with_alpha = np.dstack([bbox_array, alpha_channel])\n","\n","    # Convert the RGBA array to bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array_with_alpha)\n","\n","    # Send the overlay with the bounding box back to the frontend\n","    bbox = bbox_bytes\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"LcXd3QtuPGcy","outputId":"555c1e52-d93d-4ead-d3b7-8d59302d61b2","executionInfo":{"status":"error","timestamp":1738505820587,"user_tz":-360,"elapsed":1777,"user":{"displayName":"Abdullah Al Noman","userId":"14829655779709154281"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded Class Labels: {0: 'afif', 1: 'noman', 2: 'sabbir'}\n","Loaded Labels: {0: 'afif', 1: 'noman', 2: 'sabbir'}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        var video;\n","        var div = null;\n","        var stream;\n","        var captureCanvas;\n","        var imgElement;\n","        var labelElement;\n","\n","        var pendingResolve = null;\n","        var shutdown = false;\n","\n","        function removeDom() {\n","            stream.getVideoTracks()[0].stop();\n","            video.remove();\n","            div.remove();\n","            video = null;\n","            div = null;\n","            stream = null;\n","            imgElement = null;\n","            captureCanvas = null;\n","            labelElement = null;\n","        }\n","\n","        function onAnimationFrame() {\n","            if (!shutdown) {\n","                window.requestAnimationFrame(onAnimationFrame);\n","            }\n","            if (pendingResolve) {\n","                var result = \"\";\n","                if (!shutdown) {\n","                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","                    result = captureCanvas.toDataURL('image/jpeg', 0.8);\n","                }\n","                var lp = pendingResolve;\n","                pendingResolve = null;\n","                lp(result);\n","            }\n","        }\n","\n","        async function createDom() {\n","            if (div !== null) {\n","                return stream;\n","            }\n","\n","            div = document.createElement('div');\n","            div.style.border = '2px solid black';\n","            div.style.padding = '3px';\n","            div.style.width = '100%';\n","            div.style.maxWidth = '600px';\n","            document.body.appendChild(div);\n","\n","            const modelOut = document.createElement('div');\n","            modelOut.innerHTML = \"Status:\";\n","            labelElement = document.createElement('span');\n","            labelElement.innerText = 'No data';\n","            labelElement.style.fontWeight = 'bold';\n","            modelOut.appendChild(labelElement);\n","            div.appendChild(modelOut);\n","\n","            video = document.createElement('video');\n","            video.style.display = 'block';\n","            video.width = div.clientWidth - 6;\n","            video.setAttribute('playsinline', '');\n","            video.onclick = () => { shutdown = true; };\n","            stream = await navigator.mediaDevices.getUserMedia(\n","                {video: { facingMode: \"environment\"}});\n","            div.appendChild(video);\n","\n","            imgElement = document.createElement('img');\n","            imgElement.style.position = 'absolute';\n","            imgElement.style.zIndex = 1;\n","            imgElement.onclick = () => { shutdown = true; };\n","            div.appendChild(imgElement);\n","\n","            const instruction = document.createElement('div');\n","            instruction.innerHTML =\n","                '' +\n","                'When finished, click \"S\" to stop this demo';\n","            div.appendChild(instruction);\n","            instruction.onclick = () => { shutdown = true; };\n","\n","            // Add key press event listener\n","            window.addEventListener('keydown', function(e) {\n","                if (e.key === 's' || e.key === 'S') {\n","                    shutdown = true;  // Stop the video stream on 'S' key press\n","                }\n","            });\n","\n","            video.srcObject = stream;\n","            await video.play();\n","\n","            captureCanvas = document.createElement('canvas');\n","            captureCanvas.width = 640;\n","            captureCanvas.height = 480;\n","            window.requestAnimationFrame(onAnimationFrame);\n","\n","            return stream;\n","        }\n","\n","        async function stream_frame(label, imgData) {\n","            if (shutdown) {\n","                removeDom();\n","                shutdown = false;\n","                return '';\n","            }\n","\n","            var preCreate = Date.now();\n","            stream = await createDom();\n","\n","            var preShow = Date.now();\n","            if (label != \"\") {\n","                labelElement.innerHTML = label;\n","            }\n","\n","            if (imgData != \"\") {\n","                var videoRect = video.getClientRects()[0];\n","                imgElement.style.top = videoRect.top + \"px\";\n","                imgElement.style.left = videoRect.left + \"px\";\n","                imgElement.style.width = videoRect.width + \"px\";\n","                imgElement.style.height = videoRect.height + \"px\";\n","                imgElement.src = imgData;\n","            }\n","\n","            var preCapture = Date.now();\n","            var result = await new Promise(function(resolve, reject) {\n","                pendingResolve = resolve;\n","            });\n","            shutdown = false;\n","\n","            return {'create': preShow - preCreate,\n","                    'show': preCapture - preShow,\n","                    'capture': Date.now() - preCapture,\n","                    'img': result};\n","        }\n","    "]},"metadata":{}},{"output_type":"error","ename":"MessageError","evalue":"NotAllowedError: Permission denied","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-4c845c8fd743>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mjs_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjs_reply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-4c845c8fd743>\u001b[0m in \u001b[0;36mvideo_frame\u001b[0;34m(label, bbox)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream_frame(\"{}\", \"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: NotAllowedError: Permission denied"]}]}]}